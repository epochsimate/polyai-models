{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "Profiling (timing) results for USE, ConveRT and BERT.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/PolyAI-LDN/polyai-models/blob/master/examples/profiling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "4IObd-Vb0uXq",
    "colab_type": "code",
    "outputId": "ba098de2-2bb9-4df2-a6fa-9035474ea104",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_gpu==1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
      "\u001b[K     |████████████████████████████████| 377.0MB 45kB/s \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (0.34.2)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (0.8.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (0.9.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
      "\u001b[K     |████████████████████████████████| 491kB 41.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.27.1)\n",
      "Collecting tensorboard<1.15.0,>=1.14.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 35.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.12.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.17.5)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.0.8)\n",
      "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (0.2.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (0.1.8)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow_gpu==1.14.0) (1.1.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14.0) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14.0) (3.2.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow_gpu==1.14.0) (45.1.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow_gpu==1.14.0) (2.8.0)\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.14.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.14.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow-gpu\n",
      "  Found existing installation: tensorflow-estimator 1.15.1\n",
      "    Uninstalling tensorflow-estimator-1.15.1:\n",
      "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
      "  Found existing installation: tensorboard 1.15.0\n",
      "    Uninstalling tensorboard-1.15.0:\n",
      "      Successfully uninstalled tensorboard-1.15.0\n",
      "Successfully installed tensorboard-1.14.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0\n",
      "Collecting tensorflow_text==0.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/85/46411195f3961f4906c7e519d7a5cf017882d59149393d7c311e97cf7080/tensorflow_text-0.1.0-cp36-cp36m-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K     |████████████████████████████████| 6.2MB 7.1MB/s \n",
      "\u001b[?25hInstalling collected packages: tensorflow-text\n",
      "Successfully installed tensorflow-text-0.1.0\n",
      "Collecting tf_sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/0e/74a98e395470d94191517e7dc08921e033db8ad8ce013d62588d4bd1ad52/tf_sentencepiece-0.1.85-py2.py3-none-manylinux1_x86_64.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 6.9MB/s \n",
      "\u001b[?25hInstalling collected packages: tf-sentencepiece\n",
      "Successfully installed tf-sentencepiece-0.1.85\n",
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 4.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.12.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_gpu==1.14.0\n",
    "!pip install --no-deps tensorflow_text==0.1.0\n",
    "!pip install tf_sentencepiece\n",
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab_type": "code",
    "outputId": "b3130a52-650c-4885-af36-24024ef13e05",
    "id": "K4BZ-9R7iEjJ",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491.0
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow as tf\n",
    "import tensorflow_text  # required for tokenization ops\n",
    "import tf_sentencepiece\n",
    "\n",
    "sess = None\n",
    "\n",
    "assert tf.__version__ == \"1.14.0\", (\n",
    "    f\"found tf version {tf.__version__}, but need 1.14.0\")\n",
    "if not tf.test.is_gpu_available(): \n",
    "    print(\"Profiling on CPU. To profile on GPU please use a GPU runtime. Edit \"\n",
    "    \"-> Notebook settings -> Hardware accelerator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "HHZ6zaIo03FT",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "if not tf.test.is_gpu_available(): \n",
    "    print(\"Profiling on CPU. To profile on GPU please use a GPU runtime. Edit \"\n",
    "    \"-> Notebook settings -> Hardware accelerator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "p5XYGd3h1GoR",
    "colab_type": "code",
    "outputId": "1dd1eef7-0ffb-405c-8010-9723763affde",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConveRT encodes text to 1024-dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "# Load the tensorflow hub module into a fresh session.\n",
    "\n",
    "if sess is not None:\n",
    "    sess.close()\n",
    "\n",
    "sess = tf.InteractiveSession(graph=tf.Graph())\n",
    "\n",
    "module = tfhub.Module(\"http://models.poly-ai.com/convert/v1/model.tar.gz\")\n",
    "\n",
    "text_placeholder = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "encoding_tensor = module(text_placeholder)\n",
    "encoding_dim = int(encoding_tensor.shape[1])\n",
    "print(f\"ConveRT encodes text to {encoding_dim}-dimensional vectors\")\n",
    "\n",
    "\n",
    "def convert_encode(texts):\n",
    "    \"\"\"Encode the given texts to the encoding space.\"\"\"\n",
    "    return sess.run(encoding_tensor, feed_dict={text_placeholder: texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "MNZ0m5oX1Y-K",
    "colab_type": "code",
    "outputId": "b5fb8de1-4c31-4e76-8cef-9552fad687df",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE encodes text to 512-dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "use_module = tfhub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/1\")\n",
    "\n",
    "use_encoding_tensor = use_module(text_placeholder)\n",
    "use_encoding_dim = int(use_encoding_tensor.shape[1])\n",
    "print(f\"USE encodes text to {use_encoding_dim}-dimensional vectors\")\n",
    "\n",
    "\n",
    "def use_encode(texts):\n",
    "    \"\"\"Encode the given texts to the encoding space.\"\"\"\n",
    "    return sess.run(use_encoding_tensor, feed_dict={text_placeholder: texts})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "NGJimpK_3xF3",
    "colab_type": "code",
    "outputId": "761e27c4-0178-4db8-fd12-7687844543b3",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190.0
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT encodes text to 1024-dimensional vectors\n"
     ]
    }
   ],
   "source": [
    "from bert.tokenization import FullTokenizer\n",
    "\n",
    "uri = \"https://tfhub.dev/google/bert_uncased_L-24_H-1024_A-16/1\"\n",
    "\n",
    "embed_fn = tfhub.Module(uri, trainable=False)\n",
    "\n",
    "tokenization_info = embed_fn(\n",
    "            signature=\"tokenization_info\", as_dict=True)\n",
    "with tf.Session() as s:\n",
    "    vocab_file, do_lower_case = s.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"]\n",
    "        ])\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "input_ids_placeholder = tf.placeholder(\n",
    "    name=\"input_ids\", shape=[None, None], dtype=tf.int32)\n",
    "input_mask_placeholder = tf.placeholder(\n",
    "    name=\"input_mask\", shape=[None, None], dtype=tf.int32)\n",
    "segment_ids = tf.zeros_like(input_ids_placeholder)\n",
    "bert_inputs = dict(\n",
    "    input_ids=input_ids_placeholder,\n",
    "    input_mask=input_mask_placeholder,\n",
    "    segment_ids=segment_ids\n",
    ")\n",
    "\n",
    "bert_encoding_tensor = embed_fn(\n",
    "    inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "    \"sequence_output\"\n",
    "]\n",
    "mask = tf.expand_dims(\n",
    "    tf.cast(input_mask_placeholder, dtype=tf.float32), -1)\n",
    "bert_encoding_tensor = tf.reduce_sum(mask * bert_encoding_tensor, axis=1)\n",
    "\n",
    "bert_encoding_dim = int(bert_encoding_tensor.shape[1])\n",
    "print(f\"BERT encodes text to {bert_encoding_dim}-dimensional vectors\")\n",
    "\n",
    "def _feed_dict(texts, max_seq_len=128):\n",
    "    \"\"\"Create a feed dict for feeding the texts as input.\n",
    "    This uses dynamic padding so that the maximum sequence length is the\n",
    "    smaller of `max_seq_len` and the longest sequence actually found in the\n",
    "    batch. (The code in `bert.run_classifier` always pads up to the maximum\n",
    "    even if the examples in the batch are all shorter.)\n",
    "    \"\"\"\n",
    "    all_ids = []\n",
    "    for text in texts:\n",
    "        tokens = [\"[CLS]\"] + tokenizer.tokenize(text)\n",
    "\n",
    "        # Possibly truncate the tokens.\n",
    "        tokens = tokens[:(max_seq_len - 1)]\n",
    "        tokens.append(\"[SEP]\")\n",
    "        ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        all_ids.append(ids)\n",
    "\n",
    "    max_seq_len = max(map(len, all_ids))\n",
    "\n",
    "    input_ids = []\n",
    "    input_mask = []\n",
    "    for ids in all_ids:\n",
    "        mask = [1] * len(ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(ids) < max_seq_len:\n",
    "            ids.append(0)\n",
    "            mask.append(0)\n",
    "\n",
    "        input_ids.append(ids)\n",
    "        input_mask.append(mask)\n",
    "\n",
    "    return {input_ids_placeholder: input_ids, input_mask_placeholder: input_mask}\n",
    "\n",
    "def bert_encode(texts):\n",
    "  \"\"\"Encode the given texts to the encoding space.\"\"\"\n",
    "  return sess.run(bert_encoding_tensor, feed_dict=_feed_dict(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "x9CB_ahp1cuU",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "sess.run(tf.tables_initializer())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "XTCkDDTj1hA1",
    "colab_type": "code",
    "colab": {}
   },
   "outputs": [],
   "source": [
    "test_sentences = \"\"\"An encoder is a device, circuit, transducer, software program,\n",
    " algorithm or person that converts information from one format or code to another. \n",
    " A compressor encodes data (e.g., audio/video/images) into a smaller form (see codec) \n",
    " An audio encoder converts analog audio to digital audio signals\n",
    "A video encoder converts analog video to digital video signals An 8b/10b encoder \n",
    "creates DC balance on a communication transmission line A simple encoder or simply \n",
    "an encoder in digital electronics is a one-hot to binary converter. A rotary encoder \n",
    "converts rotary position to an analog (e.g., analog quadrature)\n",
    "or digital (e.g., digital quadrature, 32-bit parallel, or USB) electronic signal.\n",
    " That is, if there are 2n input lines, and at most only one of them will ever be \n",
    " high, the binary code of this 'hot' line is produced on the n-bit output lines. \n",
    " The illustrated gate level example implements the simple encoder defined by \n",
    "the truth table, but it must be understood that for all the non-explicitly defined\n",
    " input combinations. It is the reverse of a Decoder in its function.\n",
    " To sum up, encoders are great. Specially ConveRT.\"\"\".split(\"\\n\")\n",
    "\n",
    "import time\n",
    "\n",
    "def profile_encoder(fn):\n",
    "  t0 = None\n",
    "  n = 0\n",
    "  for i in range(100):\n",
    "    if i < 2:\n",
    "      # warmup\n",
    "      fn(test_sentences)\n",
    "      continue\n",
    "    if t0 is None:\n",
    "      t0 = time.time()\n",
    "    fn(test_sentences)\n",
    "    n += 1\n",
    "  t1 = time.time()\n",
    "  print((n * len(test_sentences)) / (t1 - t0), \"sentences/sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "k8hn-5Ph1lNB",
    "colab_type": "code",
    "outputId": "ff1e0769-8d21-4000-a3c1-fd22932023da",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.07364994530965 calls/sec\n"
     ]
    }
   ],
   "source": [
    "profile_encoder(use_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "sHNuSqlv1ogn",
    "colab_type": "code",
    "outputId": "56176ad6-fccf-49db-a0b0-9cc6dc96338e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.68791757054792 calls/sec\n"
     ]
    }
   ],
   "source": [
    "profile_encoder(convert_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "id": "yNMi0HUjAwdG",
    "colab_type": "code",
    "outputId": "0129ec09-db40-4c53-bac0-9c5f9a552bde",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34.0
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.960607323539552 calls/sec\n"
     ]
    }
   ],
   "source": [
    "profile_encoder(bert_encode)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Profiling -- USE, ConveRT, BERT",
   "provenance": [],
   "collapsed_sections": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
